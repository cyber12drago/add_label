{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import collections\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  password=\"\",\n",
    "  database=\"tugasakhir\"\n",
    ")\n",
    "\n",
    "mycursor = mydb.cursor()\n",
    "query = \"Select * from news;\"\n",
    "df = pd.read_sql(query,mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\asus-\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\asus-\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\asus-\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# \\r and \\n\n",
    "df['title_parsed_1'] = df['title'].str.replace(\"\\r\", \" \")\n",
    "df['title_parsed_1'] = df['title_parsed_1'].str.replace(\"\\n\", \" \")\n",
    "df['title_parsed_1'] = df['title_parsed_1'].str.replace(\"    \", \" \")\n",
    "\n",
    "# \" when quoting text\n",
    "df['title_parsed_1'] = df['title_parsed_1'].str.replace('\"', '')\n",
    "# Lowercasing the text\n",
    "df['title_parsed_2'] = df['title_parsed_1'].str.lower()\n",
    "punctuation_signs = list(\"?:!.,;-\")\n",
    "df['title_parsed_3'] = df['title_parsed_2']\n",
    "\n",
    "for punct_sign in punctuation_signs:\n",
    "    df['title_parsed_3'] = df['title_parsed_3'].str.replace(punct_sign, '')\n",
    "df['title_parsed_4'] = df['title_parsed_3'].str.replace(\"'s\", \"\")\n",
    "# Downloading punkt and wordnet from NLTK\n",
    "nltk.download('punkt')\n",
    "print(\"------------------------------------------------------------\")\n",
    "nltk.download('wordnet')\n",
    "# Saving the lemmatizer into an object\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nrows = len(df)\n",
    "lemmatized_text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = df.loc[row]['title_parsed_4']\n",
    "    text_words = text.split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)\n",
    "df['title_parsed_5'] = lemmatized_text_list\n",
    "\n",
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS= stopwords.words('Indonesian')\n",
    "STOPWORDS.extend(['covid','covid-19','covid-19,','korona','2020','corona', 'corona,','2021','0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','ribu','juta','-'])\n",
    "stop_words = list(STOPWORDS) #membuang kata yang tidak digunakan\n",
    "df['title_parsed_6'] = df['title_parsed_5']\n",
    "\n",
    "for stop_word in stop_words:\n",
    "\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df['title_parsed_6'] = df['title_parsed_6'].str.replace(regex_stopword, '')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24769, 300)\n",
      "(4371, 300)\n",
      "# 'criticism' label:\n",
      "  . Most correlated unigrams:\n",
      ". jokowi\n",
      ". anies\n",
      ". wabah\n",
      ". masyarakat\n",
      ". pemerintah\n",
      "  . Most correlated bigrams:\n",
      ". new normal\n",
      ". pemprov dki\n",
      "\n",
      "# 'donation' label:\n",
      "  . Most correlated unigrams:\n",
      ". medis\n",
      ". terdampak\n",
      ". rp\n",
      ". bantu\n",
      ". bantuan\n",
      "  . Most correlated bigrams:\n",
      ". pemprov dki\n",
      ". tenaga medis\n",
      "\n",
      "# 'hoax' label:\n",
      "  . Most correlated unigrams:\n",
      ". wali\n",
      ". video\n",
      ". viral\n",
      ". klaim\n",
      ". fakta\n",
      "  . Most correlated bigrams:\n",
      ". update indonesia\n",
      ". wali kota\n",
      "\n",
      "# 'notification of information' label:\n",
      "  . Most correlated unigrams:\n",
      ". meninggal\n",
      ". bertambah\n",
      ". total\n",
      ". sembuh\n",
      ". update\n",
      "  . Most correlated bigrams:\n",
      ". update indonesia\n",
      ". pasien positif\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_columns = [\"title\",\"title_parsed_6\", \"news_portal\", \"url\", \"img_url\", \"date\",\"content\",\"tag\",\"area\",\"kota\",\"label\"]\n",
    "df2 = df[list_columns]\n",
    "\n",
    "df2 = df2.rename(columns={'title_parsed_6': 'title_parsed'})\n",
    "label_codes = {\n",
    "    'notification of information': 0,\n",
    "    'donation': 1,\n",
    "    'criticism': 2,\n",
    "    'hoax': 3,\n",
    "}\n",
    "# Category mapping\n",
    "df2['label_code'] = df2['label']\n",
    "df2 = df2.replace({'label_code':label_codes})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2['title_parsed'], \n",
    "                                                    df2['label_code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)\n",
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300\n",
    "\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "for Product, label_id in sorted(label_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == label_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' label:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>news_portal</th>\n",
       "      <th>url</th>\n",
       "      <th>img_url</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>tag</th>\n",
       "      <th>area</th>\n",
       "      <th>kota</th>\n",
       "      <th>label</th>\n",
       "      <th>...</th>\n",
       "      <th>content_parsed_3</th>\n",
       "      <th>content_parsed_4</th>\n",
       "      <th>content_parsed_5</th>\n",
       "      <th>content_parsed_6</th>\n",
       "      <th>title_parsed_1</th>\n",
       "      <th>title_parsed_2</th>\n",
       "      <th>title_parsed_3</th>\n",
       "      <th>title_parsed_4</th>\n",
       "      <th>title_parsed_5</th>\n",
       "      <th>title_parsed_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jokowi: Tahun 2020 Adalah Krisis Terberat dal...</td>\n",
       "      <td>kompas</td>\n",
       "      <td>https://nasional.kompas.com/read/2020/12/31/20...</td>\n",
       "      <td>https://asset.kompas.com/crops/hmeFBt0OeWl7gV-...</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>- menyebut 2020 sebagai tahun ujian yang sanga...</td>\n",
       "      <td>Presiden Joko Widodo, Covid-19, pandemi</td>\n",
       "      <td>indonesia</td>\n",
       "      <td></td>\n",
       "      <td>criticisms</td>\n",
       "      <td>...</td>\n",
       "      <td>- menyebut 2020 sebagai tahun ujian yang sanga...</td>\n",
       "      <td>- menyebut 2020 sebagai tahun ujian yang sanga...</td>\n",
       "      <td>- menyebut 2020 sebagai tahun ujian yang sanga...</td>\n",
       "      <td>- menyebut 2020 sebagai tahun ujian yang sanga...</td>\n",
       "      <td>Jokowi: Tahun 2020 Adalah Krisis Terberat dal...</td>\n",
       "      <td>jokowi: tahun 2020 adalah krisis terberat dal...</td>\n",
       "      <td>jokowi tahun 2020 adalah krisis terberat dala...</td>\n",
       "      <td>jokowi tahun 2020 adalah krisis terberat dala...</td>\n",
       "      <td>jokowi tahun 2020 adalah krisis terberat dala...</td>\n",
       "      <td>jokowi    krisis terberat  sejarah dunia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title news_portal  \\\n",
       "0   Jokowi: Tahun 2020 Adalah Krisis Terberat dal...      kompas   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://nasional.kompas.com/read/2020/12/31/20...   \n",
       "\n",
       "                                             img_url        date  \\\n",
       "0  https://asset.kompas.com/crops/hmeFBt0OeWl7gV-...  2020-12-31   \n",
       "\n",
       "                                             content  \\\n",
       "0  - menyebut 2020 sebagai tahun ujian yang sanga...   \n",
       "\n",
       "                                       tag       area kota       label  ...  \\\n",
       "0  Presiden Joko Widodo, Covid-19, pandemi  indonesia       criticisms  ...   \n",
       "\n",
       "                                    content_parsed_3  \\\n",
       "0  - menyebut 2020 sebagai tahun ujian yang sanga...   \n",
       "\n",
       "                                    content_parsed_4  \\\n",
       "0  - menyebut 2020 sebagai tahun ujian yang sanga...   \n",
       "\n",
       "                                    content_parsed_5  \\\n",
       "0  - menyebut 2020 sebagai tahun ujian yang sanga...   \n",
       "\n",
       "                                    content_parsed_6  \\\n",
       "0  - menyebut 2020 sebagai tahun ujian yang sanga...   \n",
       "\n",
       "                                      title_parsed_1  \\\n",
       "0   Jokowi: Tahun 2020 Adalah Krisis Terberat dal...   \n",
       "\n",
       "                                      title_parsed_2  \\\n",
       "0   jokowi: tahun 2020 adalah krisis terberat dal...   \n",
       "\n",
       "                                      title_parsed_3  \\\n",
       "0   jokowi tahun 2020 adalah krisis terberat dala...   \n",
       "\n",
       "                                      title_parsed_4  \\\n",
       "0   jokowi tahun 2020 adalah krisis terberat dala...   \n",
       "\n",
       "                                      title_parsed_5  \\\n",
       "0   jokowi tahun 2020 adalah krisis terberat dala...   \n",
       "\n",
       "                              title_parsed_6  \n",
       "0   jokowi    krisis terberat  sejarah dunia  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
